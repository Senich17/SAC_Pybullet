# SAC_Pybullet
Experiments with hyper parameters

Обучение агента действовать в средах с непрерывным пространством действий, используя алгоритм SAC (Soft Actor Critic) или PPO (Proximal Policy Optimization).

### Алгоритм проведения эксперимента

1. Обучение алгоритма с гиперпараметрами по умолчанию (использован CleanRL).
2. Выбор гиперпараметров. В данном случае - коэффициент энтропии, скорость обучения и количество шагов градиентного спуска, так как они напрямую влияют на процесс обучения и производительность модели. Тестирование различных значений этих параметров позволяет найти оптимальные настройки для модели и ее обучения.
3. Собственно проведение эксперимента с матрицей выбранных гиперпараметров.
4. Визуализация ([https://wandb.ai/senich17/SAC_Pybullet?nw=nwusersenich17])

### Вариации гиперпараметров и их влияние на поведение модели

1. Запуск 0 с гиперпараметрами по умолчанию подходят для большинства задач и уже показали свою эффективность.
2. Запуск 1: learning_rate=0.001, ent_coef=0.1, gradient_steps=2.
   Модель показывает лучшие результаты по среднему вознаграждению, что может указывать на более эффективное обучение модели, но     при этом обучение идет значительно медленнее. Интересно, что в процессе обучения заметны скачки на графике вознаграждения, при    том, довольно сильные, вполне вероятно, что там можно наблюдать ту самую нестабильность.
3. Запуск 2: learning_rate=0.0001, ent_coef=0.1, gradient_steps=3.
   Новый запуск показывает улучшение производительности по среднему вознаграждению за эпизод, однако увеличение времени обучения    может оказать влияние на общую эффективность модели, тем не менее модель остается довольно стабильной.
4. Запуск 3: ent_coef=0.5.
   Отличается от предыдущих запусков очень сильно, модель как будто перестает получать вознаграждения и уделяет больше времени на    исследования.
   Важно отметить, что при использовании высокого значения коэффициента энтропии необходимо обеспечить достаточное количество     обновлений модели, чтобы стабилизировать обучение и предотвратить возможное разрушение стратегии, что доказывают показатели    этого запуска.

### Итог

Опираясь на визуализацию, ознакомиться с которой можно в ноутбуке [https://colab.research.google.com/drive/1dLI0-pR5mbzEsuXZp3u0CEUAx57vpDJi?usp=sharing], возможно оценить распределение вознаграждения на каждом этапе эксперимента.

При подборе гиперпараметров важно учитывать стабильность обучения в целом, агент должен быть надежен. Помимо этого - внимательно следить за тем, насколько ресурсоемкой становится модель и сколько времени занимает ее обучение.

   

